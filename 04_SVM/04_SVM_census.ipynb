{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e4f49a-d7a9-4575-82f6-478997fb939a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "import folium, h3, json\n",
    "from shapely.geometry import Polygon, Point\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from folium.plugins import Fullscreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f89d20-c1f7-464d-9769-ba36899b4550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df_final = pd.read_csv('features_hex_6.csv')\n",
    "\n",
    "# load the census tract areas as a geodataframe\n",
    "census_gdf = gpd.read_file('Boundaries.geojson', crs='epsg:4326')\n",
    "\n",
    "# load the spatial feature map as a geodataframe\n",
    "spatial_features = gpd.read_file('census_spatial_features.geojson', crs='epsg:4326')\n",
    "\n",
    "# Load the weather dataset\n",
    "weather = pd.read_csv('Weather_chic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf911ff2-44c7-4c46-894f-fd97fc7983c7",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style =\"font-size: 30px; font-weight: bold; color: #8EB944\">\n",
    "        Define helper functions\n",
    "    </span>\n",
    "    \n",
    "<hr style=\"color: #8EB944; height: 3px;background-color: #8EB944;border: none\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b215f1-6c81-4fb3-8de9-8f024afe4b11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_svr_grid_search(X_train: pd.DataFrame, y_train: pd.Series, use_rbf_kernel: bool = False) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Perform grid search to tune the hyperparameters of an SVR model using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features.\n",
    "    - y_train (pd.Series): Training target values.\n",
    "    - use_rbf_kernel (bool): If True, include gamma in the parameter grid for RBF kernel.\n",
    "\n",
    "    Returns:\n",
    "    - GridSearchCV: The fitted GridSearchCV object with the best parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the base parameter grid for SVR hyperparameters\n",
    "    param_grid = {\n",
    "        'svr__C': [0.1, 1, 10, 100],\n",
    "        'svr__epsilon': [0.1, 0.5, 1, 5],\n",
    "        'svr__kernel': ['linear', 'rbf', 'poly']\n",
    "    }\n",
    "    \n",
    "    # Add gamma to the parameter grid if RBF kernel is specified\n",
    "    if use_rbf_kernel:\n",
    "        param_grid['svr__gamma'] = [0.0001, 0.001, 0.005]\n",
    "\n",
    "    # Create a pipeline that first scales the data then applies SVR\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Data scaling step\n",
    "        ('svr', SVR())                 # SVR model step\n",
    "    ])\n",
    "\n",
    "    # Set up GridSearchCV with cross-validation and parallel processing\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        refit=True,     # Refit the model with the best parameters\n",
    "        verbose=1,      # Print detailed progress\n",
    "        n_jobs=-1,      # Use all available processors\n",
    "        cv=3            # Number of cross-validation folds\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best hyperparameters found\n",
    "    print(f\"Optimal Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df6e902-3ccd-436e-8290-1ec884a6d40c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_svm_kernel(X_train, y_train, X_test, kernel_type: str, C=1, epsilon=0.1, degree=3, gamma='scale'):\n",
    "    \"\"\"\n",
    "    Train an SVM model with a specified kernel and evaluate its performance.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature data.\n",
    "    - y_train: Training target data.\n",
    "    - X_test: Testing feature data.\n",
    "    - kernel_type (str): The type of kernel to use ('poly', 'linear', 'rbf', 'sigmoid').\n",
    "    - C (float): Regularization parameter. Default is 1.\n",
    "    - epsilon (float): Epsilon in the epsilon-SVR model. Default is 0.1.\n",
    "    - degree (int): Degree of the polynomial kernel function ('poly'). Ignored by other kernels. Default is 3.\n",
    "    - gamma (str): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Default is 'scale'.\n",
    "\n",
    "    Returns:\n",
    "    - svr_model: The trained SVM model.\n",
    "    - scaler: The scaler used to standardize the training and testing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate the kernel type\n",
    "    if kernel_type not in ['poly', 'linear', 'rbf', 'sigmoid']:\n",
    "        raise ValueError(\"kernel_type must be one of 'poly', 'linear', 'rbf', or 'sigmoid'\")\n",
    "    \n",
    "    # Standardize the training and testing data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Initialize the SVR model based on the kernel type\n",
    "    if kernel_type in ['linear', 'sigmoid']:\n",
    "        svr_model = SVR(kernel=kernel_type, C=C, epsilon=epsilon)\n",
    "    elif kernel_type == 'poly':\n",
    "        svr_model = SVR(kernel=kernel_type, C=C, epsilon=epsilon, degree=degree)\n",
    "    elif kernel_type == 'rbf':\n",
    "        svr_model = SVR(kernel=kernel_type, C=C, epsilon=epsilon, gamma=gamma)\n",
    "\n",
    "    # Train the SVM model\n",
    "    svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = svr_model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(f'RESULTS of {kernel_type}-kernel:\\n-----------------------------------------')\n",
    "    print(\"Mean Absolute Error (MAE):       \", round(mae, 2))\n",
    "    print(\"Mean Squared Error (MSE):        \", round(mse, 2))\n",
    "    print(\"Root Mean Squared Error (RMSE):  \", round(rmse, 2))\n",
    "    print(\"R-squared (R²):                  \", round(r2, 4))\n",
    "    print('-----------------------------------------')\n",
    "\n",
    "    # Return the trained model and the scaler\n",
    "    return svr_model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299abcff-d703-4209-a4b0-5b008da3a59c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stacked_svm_predict(X_test, meta_model, *kernel_models):\n",
    "    \"\"\"\n",
    "    Make predictions using pre-trained SVM kernels and a pre-trained meta-model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_test: Test features for which predictions are required.\n",
    "    - meta_model: The pre-trained meta-model that will combine the kernel predictions.\n",
    "    - kernel_models: A variable number of tuples where each tuple contains:\n",
    "        - A list of feature names corresponding to the kernel.\n",
    "        - The pre-fitted scaler for those features.\n",
    "        - The pre-trained SVM model.\n",
    "\n",
    "    Returns:\n",
    "    - predictions_df: A DataFrame containing predictions from each kernel and the final stacked model.\n",
    "    \"\"\"\n",
    "\n",
    "    # List to hold predictions from each kernel for the testing data\n",
    "    test_predictions = []\n",
    "\n",
    "    # Iterate over each provided SVM kernel, its corresponding features, and scaler\n",
    "    for features, scaler, svm_model in kernel_models:\n",
    "        # Scale the relevant features for the current kernel using the provided scaler\n",
    "        X_test_scaled = scaler.transform(X_test[features])\n",
    "\n",
    "        # Predict using the trained kernel on the scaled testing data\n",
    "        test_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "        # Store the predictions\n",
    "        test_predictions.append(test_pred)\n",
    "\n",
    "    # Stack the predictions from all kernels as input to the meta-model\n",
    "    X_meta_test = np.column_stack(test_predictions)\n",
    "\n",
    "    # Make final predictions using the meta-model on the test data\n",
    "    y_final_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "    # Create a DataFrame to store all predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Meta_Prediction': y_final_pred\n",
    "    })\n",
    "\n",
    "    # Add each kernel's prediction to the DataFrame\n",
    "    for i, (features, scaler, svm_model) in enumerate(kernel_models):\n",
    "        predictions_df[f'Kernel_{i+1}_Prediction'] = test_predictions[i]\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253a9dad-07b7-4d91-b703-a5ac3c1f3f98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prediction_map(selected_hour: int, prediction_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a folium map that visualizes predictions for a specific hour.\n",
    "\n",
    "    Parameters:\n",
    "    - selected_hour (int): The hour of the day for which predictions should be displayed (0-23).\n",
    "    - prediction_data (pd.DataFrame): The DataFrame containing prediction data, geometry, and other relevant features.\n",
    "\n",
    "    Returns:\n",
    "    - folium.Map: A Folium map object with predictions visualized for the selected hour.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate the hour input to ensure it is between 0 and 23\n",
    "    if selected_hour not in range(24):\n",
    "        raise ValueError('selected_hour must be between 0 and 23')\n",
    "        \n",
    "    # Filter the prediction data for the specified hour\n",
    "    hourly_data = prediction_data[prediction_data['hour'] == selected_hour].copy()\n",
    "    \n",
    "    # Create a log column for the colormap\n",
    "    shift = abs(hourly_data['Meta_Prediction'].min()) + 1\n",
    "    hourly_data['Meta_Prediction_Log'] = np.log1p(hourly_data['Meta_Prediction'] + shift)\n",
    "\n",
    "    # Create a Folium map to explore predictions for the selected hour\n",
    "    map_object = hourly_data.explore(\n",
    "        column='Meta_Prediction_Log',  # Column to base the color on\n",
    "        cmap='Reds',               # Color map for predictions\n",
    "        legend=True,               # Display a legend on the map\n",
    "        style_kwds=dict(color=\"black\", weight=1, opacity=0.4, fillOpacity=.9),\n",
    "        tooltip=['census', 'Meta_Prediction', 'num_hotels'],  # Tooltip columns\n",
    "        popup=True,                # Enable popups with all columns\n",
    "    )\n",
    "\n",
    "    # Add text labels on the map for each prediction\n",
    "    for _, row in hourly_data.iterrows():\n",
    "        # Calculate the centroid of the geometry to place the label\n",
    "        centroid = row['geometry'].centroid\n",
    "    \n",
    "        # Add a marker with a label showing the rounded prediction value\n",
    "        folium.Marker(\n",
    "            location=[centroid.y, centroid.x],\n",
    "            icon=folium.DivIcon(html=f\"\"\"<div style=\"font-size: 10pt; color: black\">{int(round(row['Meta_Prediction'], 0))}</div>\"\"\")\n",
    "        ).add_to(map_object)\n",
    "\n",
    "    return map_object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8eba3-a2b5-47e5-9645-cc3c9ee252cb",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style =\"font-size: 30px; font-weight: bold; color: #8EB944\">\n",
    "        Support Vector Machines\n",
    "    </span>\n",
    "    \n",
    "<hr style=\"color: #8EB944; height: 3px;background-color: #8EB944;border: none\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b8a11-aeda-448c-97c9-9bbbc07ad4f8",
   "metadata": {},
   "source": [
    "<span style =\"font-size: 16px; font-weight: bold;color: #507F7F;\">\n",
    "        Step 1: Create test train validation split\n",
    "</span>\n",
    "<hr style=\"color: #8EB944; height: 1px;background-color: #507F7F;border: none\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9fb7fc-ac75-4139-9b02-fe005a3161a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw a random sample of 10,000 rides from the dataset\n",
    "df_test = df_final.sample(n=10000, axis=0,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39cbbd6-fcfd-4898-8097-07f03cf470c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the DataSet into features (X) and dependent variable (Y)\n",
    "independent_variables = ['trip_seconds', 'trip_miles',\n",
    "       'fare', 'temp', 'precip', 'preciprob', 'snow', 'snowdepth', 'windspeed',\n",
    "        'snow_binary', 'rain_binary', \n",
    "       'min_dist_airport', 'dist_centre', 'num_stadiums', 'num_hotels',\n",
    "       'num_bars', 'airport_in_hex', 'perc_transport', 'perc_resid',\n",
    "       'perc_commerc', 'perc_open', 'hour', 'day_of_week', 'month',\n",
    "       'weekend_binary', 'bar_hours', 'morning_commuting', 'evening_commuting',\n",
    "       'bar_hours_weekend', 'morning_commuting_week',\n",
    "       'evening_commuting_week']\n",
    "\n",
    "# Define the X and Y Variables\n",
    "X = df_test[independent_variables]\n",
    "y = df_test['rides']\n",
    "\n",
    "# First split: train+val and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "# Second split: train and validation from the train+val set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9860e-f42f-4304-a3e1-06d8564ee7ba",
   "metadata": {},
   "source": [
    "<span style =\"font-size: 16px; font-weight: bold;color: #507F7F;\">\n",
    "        Step 2: Fit different kernels\n",
    "</span>\n",
    "<hr style=\"color: #8EB944; height: 1px;background-color: #507F7F;border: none\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788c148e-9321-4ae6-b387-eda4d052804d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS of poly-kernel:\n",
      "-----------------------------------------\n",
      "Mean Absolute Error (MAE):        38.11\n",
      "Mean Squared Error (MSE):         20712.73\n",
      "Root Mean Squared Error (RMSE):   143.92\n",
      "R-squared (R²):                   0.7887\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the features for poly kernel\n",
    "features_poly = ['num_hotels', 'num_bars', 'dist_centre', 'perc_transport',\n",
    "       'evening_commuting_week', 'bar_hours','hour']\n",
    "\n",
    "# Define the test and train features\n",
    "X_train_poly = X_train[features_poly]\n",
    "X_test_poly = X_test[features_poly]\n",
    "\n",
    "# Fit the model\n",
    "poly_model,scaler_poly = train_svm_kernel(X_train_poly, y_train,X_test_poly,'poly',C=10,epsilon=5,degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa0f56f-9b89-4101-aaad-633508fb1174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Optimal Parameters: {'svr__C': 10, 'svr__epsilon': 0.1, 'svr__kernel': 'poly'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;svr&#x27;, SVR())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;svr__C&#x27;: [0.1, 1, 10, 100],\n",
       "                         &#x27;svr__epsilon&#x27;: [0.1, 0.5, 1, 5],\n",
       "                         &#x27;svr__kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;svr&#x27;, SVR())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;svr__C&#x27;: [0.1, 1, 10, 100],\n",
       "                         &#x27;svr__epsilon&#x27;: [0.1, 0.5, 1, 5],\n",
       "                         &#x27;svr__kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;svr&#x27;, SVR())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('svr', SVR())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'svr__C': [0.1, 1, 10, 100],\n",
       "                         'svr__epsilon': [0.1, 0.5, 1, 5],\n",
       "                         'svr__kernel': ['linear', 'rbf', 'poly']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_svr_grid_search(X_val[features_poly],y_val,use_rbf_kernel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb199326-af5b-4462-aa36-1a2857b9b3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS of rbf-kernel:\n",
      "-----------------------------------------\n",
      "Mean Absolute Error (MAE):        52.44\n",
      "Mean Squared Error (MSE):         40133.16\n",
      "Root Mean Squared Error (RMSE):   200.33\n",
      "R-squared (R²):                   0.5905\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the features for poly kernel\n",
    "features_rbf = ['perc_transport','perc_commerc','perc_resid','perc_open','temp']\n",
    "\n",
    "# Define the test and train features\n",
    "X_train_rbf = X_train[features_rbf]\n",
    "X_test_rbf = X_test[features_rbf]\n",
    "\n",
    "# Fit the model\n",
    "rbf_model,scaler_rbf = train_svm_kernel(X_train_rbf, y_train,X_test_rbf,'rbf',C=100,epsilon=5, gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d0fef-e2dd-4d71-9ea2-3808e5e822ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    }
   ],
   "source": [
    "perform_svr_grid_search(X_val[features_rbf],y_val,use_rbf_kernel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b981fe-9ec5-4165-a678-08cad0085a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the features for poly kernel\n",
    "features_linear = ['weekend_binary','airport_in_hex','windspeed','snow_binary','min_dist_airport','num_hotels','num_bars','evening_commuting_week',\n",
    "                  'morning_commuting_week']\n",
    "\n",
    "# Define the test and train features\n",
    "X_train_linear = X_train[features_linear]\n",
    "X_test_linear = X_test[features_linear]\n",
    "\n",
    "# Fit the model\n",
    "linear_model,scaler_linear = train_svm_kernel(X_train_linear, y_train,X_test_linear,'linear',C=10,epsilon=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e697abb8-7a0d-47a8-bd76-511abadfcc95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perform_svr_grid_search(X_val[features_linear],y_val,use_rbf_kernel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c60edb-644f-4f2a-9903-b2ff9f027606",
   "metadata": {},
   "source": [
    "<span style =\"font-size: 16px; font-weight: bold;color: #507F7F;\">\n",
    "        Step 3: Fit the meta model\n",
    "</span>\n",
    "<hr style=\"color: #8EB944; height: 1px;background-color: #507F7F;border: none\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6522270-88cd-430a-adf9-b298ac14f4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the training data using the fitted scalers\n",
    "poly_prediction = poly_model.predict(scaler_poly.transform(X_train[features_poly]))\n",
    "rbf_prediction = rbf_model.predict(scaler_rbf.transform(X_train[features_rbf]))\n",
    "\n",
    "# Generate predictions for the test data using the same scalers\n",
    "poly_prediction_test = poly_model.predict(scaler_poly.transform(X_test[features_poly]))\n",
    "rbf_prediction_test = rbf_model.predict(scaler_rbf.transform(X_test[features_rbf]))\n",
    "\n",
    "# Combine predictions to create the meta model training data\n",
    "X_meta_train = np.column_stack((poly_prediction, rbf_prediction))\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Combine predictions for test data to create the meta model test data\n",
    "X_meta_test = np.column_stack((poly_prediction_test, rbf_prediction_test))\n",
    "\n",
    "# Predict using the meta-model\n",
    "y_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(f'RESULTS:\\n-----------------------------------------')\n",
    "print(\"Mean Absolute Error (MAE):       \", round(mae, 2))\n",
    "print(\"Mean Squared Error (MSE):        \", round(mse, 2))\n",
    "print(\"Root Mean Squared Error (RMSE):  \", round(rmse, 2))\n",
    "print(\"R-squared (R²):                  \", round(r2, 4))\n",
    "print('-----------------------------------------')     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba14dd-82fa-4e0a-8dda-94c30a06e5d9",
   "metadata": {},
   "source": [
    "<span style =\"font-size: 16px; font-weight: bold;color: #507F7F;\">\n",
    "        Step 4: Visualize the predictions\n",
    "</span>\n",
    "<hr style=\"color: #8EB944; height: 1px;background-color: #507F7F;border: none\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4cb30-4ecc-4267-962e-85f2bfd8a6ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>IMPORTANT:</b> This code cell creates a folium map with the prediction results. Unfortunately, the notebook would be too large (for github) to run it directly here. Therefore, we have uploaded the folium as a HTML file to github. Alternatively, you can execute the cell and run \"map_chicago\" in a different cell to display the folium map.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470e9d2-7f87-46d6-88b6-c0f1bb6d389e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hours of the day (0-23)\n",
    "hours = list(range(24))\n",
    "\n",
    "# Define the conditions for morning_commuting, bar_hours, and evening_commuting\n",
    "morning_commuting = [1 if 5 <= hour <= 10 else 0 for hour in hours]\n",
    "bar_hours = [0 if 5 <= hour <= 19 else 1 for hour in hours]\n",
    "evening_commuting = [1 if 13 <= hour <= 18 else 0 for hour in hours]\n",
    "\n",
    "# Create the DataFrame\n",
    "time_info = pd.DataFrame({\n",
    "    'hour': hours,\n",
    "    'morning_commuting': morning_commuting,\n",
    "    'bar_hours': bar_hours,\n",
    "    'evening_commuting_week': evening_commuting\n",
    "})\n",
    "\n",
    "spatial_features['temp'] = weather['temp'].median()\n",
    "\n",
    "prediction_map = spatial_features.merge(time_info,how='cross')\n",
    "\n",
    "data = stacked_svm_predict(\n",
    "    prediction_map,   # The test features DataFrame\n",
    "    meta_model,       # The meta-model for stacking\n",
    "    (features_poly, scaler_poly, poly_model),  # First kernel model\n",
    "    (features_rbf, scaler_rbf, rbf_model)  # Second kernel model\n",
    ")\n",
    "\n",
    "# Concatenate the predictions with the original DataFrame\n",
    "prediction_map = pd.concat([prediction_map, data], axis=1)\n",
    "\n",
    "map_chicago = create_prediction_map(8,prediction_map)\n",
    "fullscreen = Fullscreen(position='topleft')\n",
    "map_chicago.add_child(fullscreen)\n",
    "\n",
    "\n",
    "map_chicago.save('predictions_svm_census.html')\n",
    "#map_chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea2c04-ba57-4661-ad16-9091111f172d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210c8b6-81ce-45b8-9cff-70ed8d752f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac88d4-de78-4efc-bead-d536fb9abcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72ae45-51e6-4d03-b7a0-9229a3a42c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3df5fd-bb0b-44cc-b6fb-5f9ce48378bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26e0ed-069e-4b11-b6d5-b472bf96953e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5449b9-657f-4f95-b6fb-0bd1cc985555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52490b7-97b8-4213-9e29-9add9bf0c0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a4c52-d370-4ffd-b6ad-4c04f23806d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92b702-4cad-46e2-8e50-de7d5abacf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41e767-12f3-423c-9746-46d66473a510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
